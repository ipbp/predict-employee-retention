# -*- coding: utf-8 -*-
"""Asignment9_6925.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17CdtggSuQMVH6s7bLLzwr-K9Fh9pkHDs

Predict retention of an employee within an organization such that whether the employee will leave the company or continue with it. An
organization is only as good as its employees, and these people are the true source of its competitive advantage. 
Dataset is downloaded from
Kaggle. Link: https://www.kaggle.com/giripujar/hr-analytics

We do data exploration and visualization, after this create a logistic regression model to predict Employee Attrition Using Machine Learning &
Python.
"""

#Importing Libraries
import numpy as np
import pandas as pd
import matplotlib as plt

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# Reading the CSV file
hrd = pd.read_csv("/content/HR_comma_sep.csv")
hrd

hrd.head()

hrd.tail()

#random records
hrd.sample(10)

#shape of dataset
hrd.shape

#size of the dataset
hrd.size

#columns of the dataset
hrd.columns

#Datatypes of columns
hrd.dtypes

#check the data for null values 
hrd.isna().sum()

# check columns for null values 
hrd.isna().any()

import missingno as msno
import matplotlib.pyplot as plt
msno.bar(hrd)
plt.show()

#number of unique values for each column
hrd.nunique()

# Memory used by each column 
hrd.memory_usage()

# Minimun values of each column
hrd.min()

# Maximum value of each column 
hrd.max()

#basic information of dataset
hrd.info()

# Statistical Measures of the Dataset
dts.describe()

# Checking mean of the columns w.r.t to left columns
hrd.groupby("left").mean()

#No.of people left compared to each column 
hrd.groupby("left").count()

#Normalization of each category in left column 
hrd.left.value_counts(normalize = True)

# No of times each category repeated in number_project column
hrd.number_project.value_counts()

# Normalization of each category in number_project Column
hrd.number_project.value_counts(normalize = True)

# No.of times each category repeated in time_spend_company Column
hrd.time_spend_company.value_counts()

# Normalization of each category in time_spend_company Column
hrd.time_spend_company.value_counts(normalize = True)

# No.of times each category repeated in Work_accident Column
hrd.Work_accident.value_counts()

# Normalization of each category in Work_accident Column
hrd.Work_accident.value_counts(normalize = True)

# No.of times each category repeated in promotion_last_5years Column
hrd.promotion_last_5years.value_counts()

# Normalization of each category in promotion_last_5years Column
hrd.promotion_last_5years.value_counts(normalize = True)

# No.of times each category repeated in Department Column
hrd.Department.value_counts()

# Normalization of each category in Department Column
hrd.Department.value_counts(normalize = True)

# No.of times each category repeated in Salary Column
hrd.salary.value_counts()

# Normalization of each category in Salary Column
hrd.salary.value_counts(normalize = True)

#correlation
hrd.corr()

##pie char for left/stayed people 
cnt = hrd.left.value_counts()
plt.pie(cnt,labels=["left","stayed"], autopct="%0.0f%%")
plt.title("Left/Stayed People")
plt.show()

# Bar Chart for Left/Stayed People
cnt = hrd.left.value_counts()
cnt.plot.bar()
plt.title("Left / Stayed People")
plt.show()

# Pie Chart for Number of projects each employee done so far
cnt = hrd.number_project.value_counts()
plt.pie(cnt, labels = hrd.number_project.unique(), shadow = True, autopct = "%0.0f%%")
plt.title("Number of Projects")
plt.show()

# Bar Chart for Number of projects each employee done so far
cnt = hrd.number_project.value_counts()
cnt.plot.bar()
plt.title("Number of Projects")
plt.show()

# Pie Chart for Time Spent by the Employee
cnt = hrd.time_spend_company.value_counts()
plt.pie(cnt, labels = hrd.time_spend_company.unique(), shadow = True, autopct = "%0.0f%%")
plt.title("Time Spent")
plt.show()

# Bar Chart for Time Spent by the Employee
cnt = hrd.time_spend_company.value_counts()
cnt.plot.bar()
plt.title("Time Spent")
plt.show()

# Pie Chart for Work Accident Column
cnt = hrd.Work_accident.value_counts()
plt.pie(cnt, labels = ["Not Suffered", "Suffered"], shadow = True, autopct = "%0.0f%%")
plt.title("Work Accident")
plt.show()

# Bar Chart for Work Accident Column
cnt = hrd.Work_accident.value_counts()
cnt.plot.bar()
plt.title("Work Accident")
plt.show()

# Pie Chart for Promotion in Last 5 years Column
cnt = hrd.promotion_last_5years.value_counts()
plt.pie(cnt, labels = ["Didn't", "Got"], shadow = True, autopct = "%0.0f%%")
plt.title("Promotion in Last 5 years")
plt.show()

# Bar Chart for Promotion in Last 5 years Column
cnt = hrd.promotion_last_5years.value_counts()
cnt.plot.bar()
plt.title("Promotion in Last 5 years")
plt.show()

# Pie Chart for Salary Division
cnt = hrd.salary.value_counts()
plt.pie(cnt, labels = hrd.salary.unique(), shadow = True, autopct = "%0.0f%%")
plt.title("Salary Division")
plt.show()

# Bar Chart for Salary Category
cnt = hrd.salary.value_counts()
cnt.plot.bar()
plt.title("Salary Division")
plt.show()

# Pie Chart for Department Division
cnt = hrd.Department.value_counts()
plt.pie(cnt, labels = hrd.Department.unique(), shadow = True, autopct = "%0.0f%%")
plt.title("Department Division")
plt.show()

# Bar Chart for Department Category
cnt = hrd.Department.value_counts()
cnt.plot.bar()
plt.title("Department Division")
plt.show()

# Considering the most affected factors
mod_hrd = hrd[["satisfaction_level", "average_montly_hours", "promotion_last_5years", "salary"]]
mod_hrd.head()

# Getting dummy values for Salary
sal = pd.get_dummies(mod_hrd["salary"], prefix = "Salary")
sal

# Concatenating to main Dataframe
mod_hrd = pd.concat([mod_hrd, sal], axis = 1)
mod_hrd

# Dropping salary Column
mod_hrd = mod_hrd.drop(["salary"], axis = 1)

# Splitting Data into dependent and independent variables
x = mod_hrd.copy()
y = hrd["left"]

# Splitting the Data into train and test data
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 8, test_size = .2)

print(x_train)
print("Shape : ", x_train.shape)

print(x_test)
print("Shape : ", x_test.shape)

print(y_train)
print("Shape : ", y_train.shape)

print(y_test)
print("Shape : ", y_test.shape)

# Feeding the data to the model
log_reg_model = LogisticRegression()
log_reg_model.fit(x_train, y_train)
print("Model is ready for Prediction")

# Prediction for Test Data
y_predic = log_reg_model.predict(x_test)
print(y_predic)

# Actual Test Set Values
y_test

# Comparing Actual VS Predicted Values
test_val = list((y_test))
pred_val = list((y_predic))
df_comparison = pd.DataFrame({"Actual" : test_val , "Predicted" : pred_val})
df_comparison

# Accuracy of test data
accuracy_score(y_test, y_predic)

# Confusion Matrix
confusion_matrix(y_test, y_predic)